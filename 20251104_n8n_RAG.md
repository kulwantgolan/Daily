<img src="https://r2cdn.perplexity.ai/pplx-full-logo-primary-dark%402x.png" style="height:64px;margin-right:32px"/>

# Create step by step instructions

1. Vector Database (store chunked vectors)
Supabase.com -> start project, create new -- Database password (remember it) - wait for Project stautus to go green
2 n8n - new workflow
PDF in Google Drive node
n8n to connect to google drive ( actions - download file)- follow SETTING UP N8n has permissions to the google environment.
File - Pick up PDF  -- Execute step
Note: it is binary data ( binary tab), when node ran - automatically added another node "When clicking Execute workfloe" (click here will run the linked up nodes)

Add plus buttiong

I'm going to click on this
plus button after the Google Drive. I'm
going to type in Superbase and we can
see we have Superbase and this is where
we have like our relational tables and
stuff. But what we want to do is access
a vector store. So I'm going to click on
Superbase vector store I'm going to
click on add documents to vector store
and now what we're going to have to do
is set up our superbase credential and
also in subbase create a vector database
table. So in order to create our
Supabase Credential Setup
superbase credential I'm going to open
up this dropown and click on create new
credential and what it asks us for is a
host and a service ro secret. So all I
have to do is go back into subbase. You
can see now our project status is green
everything's healthy and on this lefth
hand side I'm going to go over all the
way down to settings. In here, I'm going
to scroll all the way down on this lefth
hand side to data API. And then right
here, this project URL is actually our
host. So, I'll click on copy. We'll go
back into edit end and we will paste the
host right there. And then in order to
get our service roll secret, we're going
to go back into Subbase. I'm going to
click on this left-hand side right here
where it says API keys. And then this
button right here, service ro secret.
You'll hit reveal. You'll copy it and
then you'll paste that into the section.
So, I just copied that key, pasted it in
here. And now if I hit save, we should
be connected to Superbase. Then it's
always best practice to name that
credential either your email or the date
or whatever it is just so you can keep
them all separate. So we've officially
connected to Superbase. And now what we
need to do is set up a table. If I click
into here, you can see that there's no
tables in our account. This may seem
technical, but it's going to be super
easy. You're just going to click on docs
right here. You're going to scroll a
little bit down where there is a quick
start for setting up your vector store.
And then once we get to this page, all
we have to do is scroll down a little
bit and copy this SQL command right here
and go back into our Subase and on the
lefth hand side, we're going to go to
the SQL editor. Once this opens up,
we're just going to click on new SQL
snippet. Paste that in there. Don't have
to change anything. And then hit run.
And this is basically going to create a
vector database for us called documents.
If you've already run this command
before and you want to create a
different table, all you have to do is
get rid of this first two lines that say
create extension vector because you've
already created the extension. And then
right here where it says create table
documents, you'll just change that name.
Instead of documents, you'll put in the
name that you want for your table. And
then you'll just have to change every
instance in the rest of the command
where you see documents. You'll put in
whatever you typed in right here for
your table name. But now, if I go on the
lefth hand side and go to table editor,
you can see that we have a table right
here called documents. and it's waiting
for us to put in our vectors. So, back
in Nitn, I can open this up and we
should see if we just refresh this list,
maybe we should see our table right here
called documents. And I don't want to
confuse you guys. So, if you followed
all the steps I just did, you won't have
to worry about this. But if you did
create a different table, let's say you
created a table called test, you would
just make sure you change this right
here to match test rather than match
documents. And now that we've connected
here, we just have to do two more
things. we have to chunk and then embed
so that we can actually vectorize. The
first thing we're going to do is chunk.
And in order to do that, we're going to
click on this plus right here under
document. This is going to open up some
document loaders for us. And I'm going
to choose default data loader. Now,
what's happening here is we're just
basically telling Subase, here's what I
want to vectorize, and here's how I want
you to chunk it up. And the good news is
we can basically just leave the text
splitting as simple. So, it splits up
every thousand characters with 200
characters of overlap. If you wanted to
get more custom, you could always create
custom, you know, 500 characters, 2,000
characters, whatever you want. But right
now, we're just going to keep it as
simple. And then one more thing that's
kind of tricky is we have to tell it
what document to look at to actually
embed. And it's set by default to JSON.
And we can't do that because our PDF is
right here. And if you'll notice at the
top right here, this PDF is being stored
as binary. So, we want to change the
type of data to binary. Now, we should
be all set up when it comes to loading
data and chunking. So now that we have
our chunking set up, we just need to
connect our embeddings model. So I'm
going to click on the plus button right
here. I'm going to click on embeddings
openai. And if you don't have an OpenAI
OpenAI API Credential Setup
account, all you have to do is click on
create new credential. And you just need
an API key. So what you have to do is go
to OpenAI API in order to make an
account. This is different than just
having a chat GBT plan because you'll
have to add billing information to your
OpenAI API account. And once you get
here, you'll just go to your dashboard.
You'll click on API keys and you'll
create a new key. And all you have to do
is copy that key and then you'll paste
it right there and you'll be connected.
And now all we have to do is choose
model text embedding 3 small. If you
followed all my steps, you should be
good to go. But if you did set up a
vector database with different
dimensions, you'll have to make sure
that those line up right here. But I
don't want to confuse you guys. If you
did what I did, then you won't have to
touch that at all. Okay, so everything
looks like it should be set up. I'm
going to hit the play button above our
vector store. And what this is doing is
it's chunking, it's embedding, and now
finally it's vectorizing just like we
saw earlier on that slide. And what you
can see is that we got 77 vectors. So
that 22page PDF that we were looking at
right here, it ended up splitting and
chunking into 77 different vectors. So
if I go into my subbase table and we
give this a sec, there we go. We should
have 77 rows of information about the
rules of golf. And if you want to see
the data that's actually in each vector,
you would look at this column called
content. And if I open this up, you can
see here's the first vector. The rules
of golf simplified rule one. You start
the game within the te and ground and
hit the ball until it ends up in the
hole. Blah blah blah. So here's this
first vector. Here's this 10th vector.
And then here is, you know, the 41st
vector. In this column, we have
metadata. And then in this column, we
have the actual numerical
representation, which is our embedding.
So we have our data in here, which was
the first step. That was the rag
pipeline. Now, let's head back into nitn
and let's create an AI agent that can
actually talk to the information in
RAG Agent Build
here. So, what I'm going to do is hit
tab and I'm going to type in AI agent
and grab that agent right there. Now, as
we talked about, the way we want to
trigger this agent is just through a
conversation. So, I'm going to grab a
chat trigger, which is right here. And
then all I have to do is hook up this
chat trigger right into the front of the
agent. And now it will be able to get
our messages when we talk to it. Now, we
need to connect a brain to our AI agent.
So, if I click on the plus under chat
model, I'm going to scroll down here and
grab OpenAI chat model just because if
you are setting all this up for the
first time, you already know how to get
there and get your API key and you
already have that credential set up. And
now we just need to choose a model to
use. I'm going to stick with GPT4.1
mini. And now our agent has a brain. And
the final step is just to give it a tool
so that it can actually look in this
vector database. So, we're going to
click on tool. We are going to type in
superbase vector store.
The operation's already set to retrieve
documents as a tool for AI agent. The
description, I'm just going to say call
this tool to look up the rules of golf.
So, this is just me telling the agent
what this tool does and when to use it.
And then once again, we just have to
choose the right table to look through,
which in our case is called documents.
And then the limit is basically how many
vectors do you want it to actually pull
back. Right now, we'll just stick with
four. And then the last thing we have to
do is actually connect another
embeddings model because as you guys
know the query has to get embedded as
well. And something that's really simple
is we already have an embeddings model
set up down here. So I can actually just
drag this into that exact same one from
down here. You don't have to do that.
You could add a new one. But just to
show you guys that they use the same
one. I'll do it like this. So without
even giving our agent any sort of
instructions or system prompt, let's
Testing RAG Agent
test this out and see how it works. All
right. All right. So, here we are in the
PDF about the rules of golf. And what
I'm going to do is just ask it basically
about the practice rules. So, I'm just
going to say, what am I allowed to do
for practice? Let's see if the agent
understands that we're talking about
golf and that it needs to use the
subbase vector store, which it did. And
now, it's going to take that information
and return an answer to us. And then
we'll basically validate if it is
correct or not. So, right here, we get
our answer. According to the rules of
golfer practice, before or between
rounds, you can practice on the course
the day of each match. You cannot
practice on the course the day of stroke
play tournament or before playoff on the
course. And also during the round, you
cannot hit a practice shot while playing
a hole or between holes. And if we go
over to the PDF, you can see that's
pretty much exactly word for word what
we were looking for. And just to show
Reading Agent Logs
you guys what actually happened behind
the scenes, you can click into the agent
and you can click on logs and this will
show us exactly what just happened. So
the human asked the AI agent, what am I
allowed to do for practice? That's what
triggered the workflow. The agent uses
the superbase vector store and sends off
the query practice rules in golf. And
then this gets embedded using the
embeddings model right here. And then
what we get back after that gets
embedded is the answer which is these
four different vectors which is the
actual content in the vector database.
So then the AI agent will look at all
this context and it will think about the
question it was asked in the first place
and then it will actually give us an
output which is what we saw down here in
the chat message section. So now that
Setting up Memory
we've validated that the AI agent can
look through the Superbase vector
database, we have one issue, which is it
has no memory. So we literally just
asked it about the rules of golf. So now
if I asked it, so I can't hit a practice
shot between holes, it should be able to
just use its memory because it just
looked that up already and answer us.
But what it's going to do is it's going
to go ahead and search the vector
database again because it's treating
each of our messages as a completely
individual unique conversation. So it's
fine because it got the answer right
again. But ideally it would be able to
look through its memory and realize we
just talked about this. I don't have to
look up anything. I can just provide
them an answer. So the way we're going
to set up that memory is we are going to
use Postgress because we've already set
up our whole soup base thing. So I'm
going to click on the plus under memory
right here. And you can see we could use
a simple memory which would store
everything in NADN. But if we want to be
able to look through the different
interactions that we're having with our
AI agent and store that in Subbase, what
we're going to do is connect to a
Postgress chat memory. So the first
thing that we actually have to do though
is connect to our Postgress credential.
So to set up this credential, I'm going
to open up the dropown, click on create
new credential, and you can see it's a
little bit different than that original
Superbase one, but just follow what I do
and you'll be all set. I'm going to go
into Superbase. And at the top of the
screen, you can see there's a connect
button. So, I'm going to click on this
and I'm just going to scroll down a
little bit where we have right here the
transaction pooler. I'm going to click
right here on view parameters and then
we just have to grab a few of these
things. So, the first one is going to be
the host. I'm going to copy the host and
we're going to go back into nitn and
just paste that in right here.
The database I can leave as postgress
because if you can see right here the
database is Postgress. But the user
we're going to copy this and go back in
and paste that in as a user.
The password is going to be the password
that you set up when you created your
project earlier in the video. So, let me
type that in real quick. And then the
last thing we need to do is the port,
which if we go back into this screen, we
can see right here we have the port
6543. So, all I have to do is come in
here, paste that there, and then if we
hit save, we should go green. Connection
tested successfully. And so once we
start chatting with our agent, once
again, it's going to store not only what
we asked the agent, but it's also going
to store the agent's response to us in a
different table in Subase. So let me
chat with the agent and then I'll show
you guys that. So now if I'm talking to
my agent and I say, "Hello, my name is
Nate." We're going to send that off.
It's going to check if we've talked to
it before. It's now going to answer us.
And we can see that if I go into
Subbase, there's going to be a new table
right here. If I just refresh this real
quick, called Niden chat histories. And
if I click into that, it's basically
going to have human content. So every
time we talk to it, it's going to store
that here. And then it's also going to
store the AI agent's response to us
every single time. And so now if I was
to come into the AI agent and say,
"What's my name?" It's going to look in
the Postgress memory and then it will be
able to answer us because it actually
has that context window. Your name is
Nate. How can I help you today? Two
things though to keep in mind real
quick. The first one is that in this
Postgress chat memory, the context
window length is five. So every time we
talk to it, it's only going to remember
the five most recent interactions. But
you have the ability to adjust this
context window length. But then the AI
would just be processing more tokens.
And then one other thing to keep in mind
in Postgress, what it's doing is it's
going to store these messages with a
session ID and that's how it looks it
up. So if we are to go into NN and clear
the session, it's going to start a new
session and it's no longer going to
remember that. So if I now say what's my
name, it's not going to be able to get
that because it's looking through a
different session ID. I don't have
access to your personal information. And
then if I go into the Superbase table
and refresh, we can see that these two
interactions that we just had were a
different session ID than these four
previous ones. So it's not a huge deal
because right now we're just chatting
with the agent using this chat trigger.
But if you were using, you know, like
Gmail or you were using text message or
something, you could store the session
ID as someone's email address or
someone's phone number. And that's how
the agent could actually retain all of
that context with that individual
person. Okay, so you've officially just
built your first rag pipeline where you
Want to Master n8n?
got data into a vector database and you
built a rag agent that can talk to the
data in there as well. There's lots of
ways to expand off of this type of
system. I'd say the next one would be
actually creating it so it's a little
bit more dynamic so that whenever you
drop a new doc into a Google Drive
folder, it would automatically get
pushed into Superbase and then creating
a second pipeline down here where
whenever you update that document, it
basically deletes the old vectors, the
outdated ones, and then it puts the new
ones into the vector database. And
that's how you can make sure the
information that the agent's actually
looking at and using to answer you is
accurate and real time and relevant.
Then another thing you want to do is
have a different system over here where
whenever a file is deleted, it's also
going to delete all those vectors from
the database so you can keep the whole
database very clean. So if you're
interested in seeing this type of build,
let me know in the comments and I'd love
to make a YouTube video to cover how
simple this would be to set up. But I
also just did a live build of this
system on my paid community. Link for
that would be down in the description.
This is a really great community of
members who are building with niten and
sharing their challenges and their wins
every single day. We've also got two
full courses in here. One's called Agent
Zero, which is like the foundations for
AI automation, and then 10 hours to 10
seconds where you learn how to identify,
design, and build time-saving
automations. Anyways, I'd love to see
you guys in this community. But that's
going to do it for this one. If you
enjoyed the video or you learned
something new, please give it a like.
Definitely helps me out a ton. And as
always, I appreciate you guys making it
to the end of the video. I'll see you
all in the next one. Thanks so much,
guys.

### Setting up google connection
SETTING UP N8n has permissions to the google environment.
On google drive node - create new credential -- There is open docs
NOte: OAUTH REDIRECT URL ( come handy later) and we need client ID and secret
Drill in open docs - prerequisite - create google cloud account
(in google cloud)
Create project and go to the project
API and Services - oAuth consent screen
oAuth copnsent screen registration ..Create ..App name, email address, developer email, add test users -- (this email is the one we got for n8n) creat
copy oAuth client ID and client secret
ALSO ENABLE API and services - google drive API - enable ( enable for other google service- GMAIL API, GOOGLE DOC API etc,)
(in n8n)
Paste  client ID and client secret ANd "Sign in Google" - pop up appears - select same account as test user and allow access
Refresh to see file


## Here are the step-by-step instructions to set up a RAG pipeline with a vector database using Supabase and n8n automation:

1. Vector Database Setup (Supabase.com)

- Start a new project on Supabase.com.
- Create a new project with a database password (remember it).
- Wait until the project status turns green indicating the project is healthy.

2. n8n Workflow Creation

- Open n8n and create a new workflow.
- Add a Google Drive node to the workflow.
- Connect n8n to "Google Driv"e with Action: "Download file". 

<img width="1234" height="700" alt="image" src="https://github.com/user-attachments/assets/d652314b-6b4e-48e1-9d0a-1ef6471972c3" />
- Click open Docs and Follow instructions in Point 10 to connect to google drive

- Select a PDF file from Google Drive in the node and execute it. Note the file is loaded as binary data.
  
3. Supabase Vector Store Integration in n8n

- After Google Drive node, click the plus button.
- Add a Supabase vector store node with the action "Add documents to vector store".
- Create a new Supabase credential in n8n by copying the project URL (host) from Supabase Data API settings and the service_ro_secret from Supabase API keys.
- Paste these values into n8n credential setup and save to connect.

4. Create Vector Database Table in Supabase

- In Supabase, go to Docs > Quick Start for vector store and copy the provided SQL command.
- Open the SQL editor in Supabase, create a new snippet, paste the SQL command, and run it to create the vector extension and a "documents" table.
- To use a different table name, remove "create extension vector" line if already created and rename the table and references inside the SQL command.

5. Chunking and Embedding in n8n

- Add a document loader node after Supabase node and select "default data loader".
- Configure text splitting to simple: 1000 characters with 200 overlap.
- Change the data input type from JSON to binary for the PDF file.
- Add an OpenAI Embeddings node, create API credentials with OpenAI API key, and choose the model "text-embedding-3-small".

6. Vectorizing Documents

- Run the workflow to chunk, embed, and vectorize the PDF content.
- Verify vector rows have appeared in the Supabase "documents" table with embedded data and metadata.

7. Build RAG AI Agent in n8n

- Add an AI agent node and a chat trigger node, connecting trigger to agent.
- Add OpenAI chat model node using the GPT-4.1 mini model and connect it to the agent.
- Add a Supabase vector store tool to the agent set to retrieve documents, and provide a description for the tool.
- Connect the query embedding model to the AI agent tool for vector searches.

8. Test the RAG AI Agent

- Interact with the agent by asking questions related to the PDF content (e.g., rules of golf).
- Validate the returned answers match the information in the original PDF.

9. Setup Memory for the AI Agent

- Add a Postgres chat memory node in n8n.
- Create a Postgres credential using host, user, database, port info, and password from Supabase Connect settings.
- Connect the memory node to the AI agent to store chat histories.
- Test memory by having multiple interactions; note the limited context window (5 interactions by default).
- Understand session ID usage for conversation continuity.

10. Google Drive Node Permissions Setup (OAuth)

- In Google Drive node, create a new credential using OAuth redirect URL.

  <img width="1032" height="629" alt="image" src="https://github.com/user-attachments/assets/95f02285-bc31-43db-98fa-e87a54d9160b" />
  
- go to https://console.cloud.google.com/
- Go to New Project (Create new project and select it)
  <img width="1463" height="553" alt="image" src="https://github.com/user-attachments/assets/46978a24-ef75-4617-9b2a-fe593e691594" />
  <img width="673" height="547" alt="image" src="https://github.com/user-attachments/assets/27971256-e92d-4ea6-8bdd-11cd3a5e4e51" />
  <img width="504" height="228" alt="image" src="https://github.com/user-attachments/assets/a0c77688-cc06-4b76-8f5f-154c5d0f9419" />

- Create a Google Cloud project, set up the OAuth consent screen with app name, emails, and test users.
  <img width="574" height="718" alt="image" src="https://github.com/user-attachments/assets/b710e05f-93e2-42d0-a1e7-bf3ecd0b16ed" />

- Enable Google Drive API and any other required APIs.
- Copy OAuth client ID and secret from Google Cloud and paste into n8n credential setup.
- Sign in with a test user account and refresh to see files.

This completes the setup of a vectorized document RAG pipeline with an AI agent that can read from and remember information stored in Supabase, with source documents pulled from Google Drive via n8n automation.

